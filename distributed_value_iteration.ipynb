{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Synchronous Value Iteration\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint, choice\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake\n",
    "\n",
    "We will use the FrozenLake environment as the MDP environment for this experiment. This is a type of gridworld environment, whose size (number of states) can be controlled by adjusting the grid dimensions. The environment is intended to model the process of navigating a frozen lake, while avoiding falling into holes with the objective of reaching a goal location. \n",
    "\n",
    "Below is the code for the FrozenLake environment class, which has the following functions that will be used: \n",
    "\n",
    "- FrozenLake.GetSuccesors() : Take a state and an action as input, and return a list of pairs, where each pair $(s',p)$ is a successor state $s'$ with non-zero probability and $p$ is the probability of transitioning to $p$.  \n",
    "\n",
    "- FrozenLake.GetTransitionProb() : Take a state, an action, a next state as input, and return the probability of the transition \n",
    "\n",
    "- FrozenLake.GetReward() : Take a state and an action as input, and return the reward of that.\n",
    "\n",
    "The version we are using for the assignment 2 is a modified version of the environment at the following location.   \n",
    "  \n",
    "Source: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py  \n",
    "\n",
    "Execute the following cell to initialize the MDP environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import numpy as np\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=sys.maxsize, precision = 2)\n",
    "TransitionProb = [0.7, 0.1, 0.1, 0.1]\n",
    "def generate_row(length, h_prob):\n",
    "    row = np.random.choice(2, length, p=[1.0 - h_prob, h_prob])\n",
    "    row = ''.join(list(map(lambda z: 'F' if z == 0 else 'H', row)))\n",
    "    return row\n",
    "\n",
    "\n",
    "def generate_map(shape):\n",
    "    \"\"\"\n",
    "\n",
    "    :param shape: Width x Height\n",
    "    :return: List of text based map\n",
    "    \"\"\"\n",
    "    h_prob = 0.1\n",
    "    grid_map = []\n",
    "\n",
    "    for h in range(shape[1]):\n",
    "\n",
    "        if h == 0:\n",
    "            row = 'SF'\n",
    "            row += generate_row(shape[0] - 2, h_prob)\n",
    "        elif h == 1:\n",
    "            row = 'FF'\n",
    "            row += generate_row(shape[0] - 2, h_prob)\n",
    "\n",
    "        elif h == shape[1] - 1:\n",
    "            row = generate_row(shape[0] - 2, h_prob)\n",
    "            row += 'FG'\n",
    "        elif h == shape[1] - 2:\n",
    "            row = generate_row(shape[0] - 2, h_prob)\n",
    "            row += 'FF'\n",
    "        else:\n",
    "            row = generate_row(shape[0], h_prob)\n",
    "\n",
    "        grid_map.append(row)\n",
    "        del row\n",
    "\n",
    "    return grid_map\n",
    "\n",
    "\n",
    "\n",
    "MAPS = {\n",
    "    \n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "    \"16x16\": [\n",
    "        \"SFFFFFFFFHFFFFHF\",\n",
    "        \"FFFFFFFFFFFFFHFF\",\n",
    "        \"FFFHFFFFHFFFFFFF\",\n",
    "        \"FFFFFFFFHFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFF\",\n",
    "        \"FFHHFFFFFFFHFFFH\",\n",
    "        \"FFFFFFFFFFFFFFFF\",\n",
    "        \"FFFFFHFFFFFFHFFF\",\n",
    "        \"FFFFFHFFFFFFFFFH\",\n",
    "        \"FFFFFFFHFFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFHFFF\",\n",
    "        \"FFFFFFHFFFFFFFFF\",\n",
    "        \"FFFFFFFFHFFFFFFF\",\n",
    "        \"FFFFFFFFFHFFFFHF\",\n",
    "        \"FFFFFFFFFFHFFFFF\",\n",
    "        \"FFFHFFFFFFFFFFFG\",\n",
    "    ],\n",
    "    \n",
    "    \"32x32\": [\n",
    "        'SFFHFFFFFFFFFFFFFFFFFFFFFFHFFFFF',\n",
    "        'FFHFHHFFHFFFFFFFFFFFFFFFFFHFFFFF',\n",
    "        'FFFHFFFFFFFFHFFHFFFFFFFFFFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFHFHHFHFHFFFFFHFFFH',\n",
    "        'FFFFHFFFFFFFFFFFFFFFHFHFFFFFFFHF',\n",
    "        'FFFFFHFFFFFFFFFFHFFFFFFFFFFHFFFF',\n",
    "        'FFHHFFFFHFFFFFFFFFFFFFFFFFFFFFFF',\n",
    "        'FFFHFFFFFFFFFFHFFFHFHFFFFFFFFHFF',\n",
    "        'FFFFHFFFFFFHFFFFHFHFFFFFFFFFFFFH',\n",
    "        'FFFFHHFHFFFFHFFFFFFFFFFFFFFFFFFF',\n",
    "        'FHFFFFFFFFFFHFFFFFFFFFFFHHFFFHFH',\n",
    "        'FFFHFFFHFFFFFFFFFFFFFFFFFFFFHFFF',\n",
    "        'FFFHFHFFFFFFFFHFFFFFFFFFFFFHFFHF',\n",
    "        'FFFFFFFFFFFFFFFFHFFFFFFFHFFFFFFF',\n",
    "        'FFFFFFHFFFFFFFFHHFFFFFFFHFFFFFFF',\n",
    "        'FFHFFFFFFFFFHFFFFFFFFFFHFFFFFFFF',\n",
    "        'FFFHFFFFFFFFFHFFFFHFFFFFFHFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFFFFFFFFFFFFHFFFFF',\n",
    "        'FFFFFFFFHFFFFFFFHFFFFFFFFFFFFFFH',\n",
    "        'FFHFFFFFFFFFFFFFFFHFFFFFFFFFFFFF',\n",
    "        'FFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFHFFFFHFFFFFFFHFFF',\n",
    "        'FFHFFFFHFFFFFFFFFHFFFFFFFFFFFHFH',\n",
    "        'FFFFFFFFFFHFFFFHFFFFFFFFFFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFFFHHFFHHHFFFHFFFF',\n",
    "        'FFFFFFFFFFFFFFHFFFFHFFFFFFFHFFFF',\n",
    "        'FFFFFFFHFFFFFFFFFFFFFFFFFFFFFFFF',\n",
    "        'FFFFFHFFFFFFFFFFFFFFFFHFFHFFFFFF',\n",
    "        'FFFFFFFHFFFFFFFFFHFFFFFFFFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFFFFFFFFFFHFFFFFFF',\n",
    "        'FFFFFFFFFFFFFFFHFFFFFFFFHFFFFFFG',\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # BFS to check that it's a valid path.\n",
    "    def is_valid(arr, r=0, c=0):\n",
    "        if arr[r][c] == 'G':\n",
    "            return True\n",
    "\n",
    "        tmp = arr[r][c]\n",
    "        arr[r][c] = \"#\"\n",
    "\n",
    "        # Recursively check in all four directions.\n",
    "        directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "        for x, y in directions:\n",
    "            r_new = r + x\n",
    "            c_new = c + y\n",
    "            if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                continue\n",
    "\n",
    "            if arr[r_new][c_new] not in '#H':\n",
    "                if is_valid(arr, r_new, c_new):\n",
    "                    arr[r][c] = tmp\n",
    "                    return True\n",
    "\n",
    "        arr[r][c] = tmp\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        rew_hole = -1000\n",
    "        rew_goal = 1000\n",
    "        rew_step = -1\n",
    "        \n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "        self.TransitProb = np.zeros((nA, nS + 1, nS + 1))\n",
    "        self.TransitReward = np.zeros((nS + 1, nA))\n",
    "        \n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "        \n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col-1,0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a == UP:\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'H':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                        self.TransitProb[a, s, nS] = 1.0\n",
    "                        self.TransitReward[s, a] = rew_hole\n",
    "                    elif letter in b'G':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                        self.TransitProb[a, s, nS] = 1.0\n",
    "                        self.TransitReward[s, a] = rew_goal\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            #for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                            for b, p in zip([a, (a+1)%4, (a+2)%4, (a+3)%4], TransitionProb):\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                #rew = float(newletter == b'G')\n",
    "                                #li.append((1.0/10.0, newstate, rew, done))\n",
    "                                if newletter == b'G':\n",
    "                                    rew = rew_goal\n",
    "                                elif newletter == b'H':\n",
    "                                    rew = rew_hole\n",
    "                                else:\n",
    "                                    rew = rew_step\n",
    "                                li.append((p, newstate, rew, done))\n",
    "                                self.TransitProb[a, s, newstate] += p\n",
    "                                self.TransitReward[s, a] = rew_step\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G')\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n",
    "    \n",
    "    def GetSuccessors(self, s, a):\n",
    "        next_states = np.nonzero(self.TransitProb[a, s, :])\n",
    "        probs = self.TransitProb[a, s, next_states]\n",
    "        return [(s,p) for s,p in zip(next_states[0], probs[0])]\n",
    "    \n",
    "    def GetTransitionProb(self, s, a, ns):\n",
    "        return self.TransitProb[a, s, ns]\n",
    "    \n",
    "    def GetReward(self, s, a):\n",
    "        return self.TransitReward[s, a]\n",
    "    \n",
    "    def GetStateSpace(self):\n",
    "        return self.TransitProb.shape[1]\n",
    "    \n",
    "    def GetActionSpace(self):\n",
    "        return self.TransitProb.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initializations\n",
    "\n",
    "Run the following cell to initilize maps of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_8 = (MAPS[\"8x8\"], 8)\n",
    "map_16 = (MAPS[\"16x16\"], 16)\n",
    "map_32 = (MAPS[\"32x32\"], 32)\n",
    "map_50 = (generate_map((50,50)), 50)\n",
    "map_110 = (generate_map((110,110)), 110)\n",
    "\n",
    "MAP = map_8\n",
    "map_size = MAP[1]\n",
    "run_time = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, trials = 1000):\n",
    "    total_reward = 0\n",
    "    for _ in range(trials):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        observation, reward, done, info = env.step(policy[0])\n",
    "        total_reward += reward\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(policy[observation])\n",
    "            total_reward += reward\n",
    "    return total_reward / trials\n",
    "\n",
    "def evaluate_policy_discounted(env, policy, discount_factor, trials = 1000):\n",
    "    total_reward = 0\n",
    "    for _ in range(trials):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        observation, reward, done, info = env.step(policy[0])\n",
    "        total_reward += reward\n",
    "        power = 0\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(policy[observation])\n",
    "            total_reward += pow(discount_factor, power) * reward\n",
    "            power += 1\n",
    "    return total_reward / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "This function shows the policy and state values and saves them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(v, pi, map_size, env, beta, name):\n",
    "    v_np, pi_np  = np.array(v), np.array(pi)\n",
    "    print(\"\\nState Value:\\n\")\n",
    "    print(np.array(v_np[:-1]).reshape((map_size,map_size)))\n",
    "    print(\"\\nPolicy:\\n\")\n",
    "    print(np.array(pi_np[:-1]).reshape((map_size,map_size)))\n",
    "    print(\"\\nAverage reward: {}\\n\".format(evaluate_policy(env, pi)))\n",
    "    print(\"Avereage discounted reward: {}\\n\".format(evaluate_policy_discounted(env, pi, discount_factor = beta)))\n",
    "    print(\"State Value image view:\\n\")\n",
    "    plt.imshow(np.array(v_np[:-1]).reshape((map_size,map_size)))\n",
    "    \n",
    "    pickle.dump(v, open(name + \"_\" + str(map_size) + \"_v.pkl\", \"wb\"))\n",
    "    pickle.dump(pi, open(name + \"_\" + str(map_size) + \"_pi.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initialize Ray\n",
    "\n",
    "Now we are going to use Ray to develop distributed versions of the above value iteration algorithm. The first step of course is to initialize Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-21 00:04:31,756\tINFO resource_spec.py:212 -- Starting Ray with 186.62 GiB memory available for workers and up to 0.93 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.9.1.6',\n",
       " 'redis_address': '10.9.1.6:17461',\n",
       " 'object_store_address': '/tmp/ray/session_2021-06-21_00-04-31_754119_15735/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-06-21_00-04-31_754119_15735/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-06-21_00-04-31_754119_15735'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(include_webui = False, ignore_reinit_error=True, redis_max_memory=100000000, object_store_memory=1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class VI_server(object):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.v_current = [0] * size\n",
    "        self.pi = [0] * size\n",
    "        self.v_new = [0] * size\n",
    "    \n",
    "    def get_value_and_policy(self):\n",
    "        return self.v_current, self.pi\n",
    "    \n",
    "    def update(self, update_index_part, update_v_part, update_pi_part):\n",
    "        for i, update_index in enumerate(update_index_part):\n",
    "            self.v_new[update_index] = update_v_part[i]\n",
    "            self.pi[update_index] = update_pi_part[i]\n",
    "    \n",
    "    def get_error_and_update(self):\n",
    "        max_error = 0\n",
    "        for i in range(len(self.v_current)):\n",
    "            error = abs(self.v_new[i] - self.v_current[i])\n",
    "            if error > max_error:\n",
    "                max_error = error\n",
    "            self.v_current[i] = self.v_new[i]\n",
    "        return max_error\n",
    "    \n",
    "@ray.remote\n",
    "def VI_worker(VI_server, data, start_state, end_state):\n",
    "    env, workers_num, beta, epsilon = data\n",
    "    A = env.GetActionSpace()\n",
    "    S = env.GetStateSpace()\n",
    "\n",
    "    successors = {}\n",
    "    for state in range(start_state, end_state):\n",
    "        for action in range(A):\n",
    "            successors[(state, action)] = env.GetSuccessors(state, action)\n",
    "\n",
    "    #get shared variable\n",
    "    partofv, _ = ray.get(VI_server.get_value_and_policy.remote())\n",
    "\n",
    "    #bellman backup\n",
    "    update_v_part = [0] * (end_state - start_state)\n",
    "    update_pi_part = [0] * (end_state - start_state)\n",
    "    update_index_part = []\n",
    "    for index in range(start_state, end_state):\n",
    "        update_index_part.append(index)\n",
    "#         update_index_list = list(range(start_state, end_state))\n",
    "\n",
    "    for index, state in enumerate(update_index_part):\n",
    "        max_v = float('-inf')\n",
    "        max_a = 0\n",
    "        for action in range(A):\n",
    "            new_v = 0\n",
    "            for n_state, prob in successors[(state, action)]:\n",
    "                new_v += beta * prob * partofv[n_state]\n",
    "            new_v += env.GetReward(state, action)\n",
    "            if new_v > max_v:\n",
    "                max_v = new_v\n",
    "                max_a = action\n",
    "        update_v_part[index] = max_v\n",
    "        update_pi_part[index] = max_a\n",
    "\n",
    "    VI_server.update.remote(update_index_part, update_v_part, update_pi_part)\n",
    "                    \n",
    "                    \n",
    "def sync_value_iteration_distributed(env, beta = 0.999, epsilon = 0.01, workers_num = 4, stop_steps = 2000):\n",
    "    S = env.GetStateSpace()\n",
    "    VI_Server = VI_server.remote(S)\n",
    "    workers_list = []\n",
    "    data_id = ray.put((env, workers_num, beta, epsilon))\n",
    "\n",
    "    # initial start states and end states list for workers \n",
    "    start_state_list = [0] * workers_num\n",
    "    end_state_list = [0] * workers_num\n",
    "    \n",
    "    # size of states for each worker\n",
    "    size = S // workers_num\n",
    "    \n",
    "    # fill lists\n",
    "    for i in range(workers_num):\n",
    "        if (i + 1) * size < S: \n",
    "            # not the last worker\n",
    "            start_state_list[i] = i * size\n",
    "            end_state_list[i] = (i + 1) * size\n",
    "        else:  \n",
    "            # the last worker\n",
    "            start_state_list[i] = i * size\n",
    "            end_state_list[i] = S\n",
    "            \n",
    "    error = float('inf')\n",
    "    while error > epsilon:\n",
    "        w_ids = []\n",
    "        for i in range(workers_num):\n",
    "            w_ids.append(VI_worker.remote(VI_Server, data_id, start_state_list[i], end_state_list[i]))\n",
    "    \n",
    "        ray.wait(w_ids, num_returns = 1)  \n",
    "        error = ray.get(VI_Server.get_error_and_update.remote())\n",
    "\n",
    "    v, pi = ray.get(VI_Server.get_value_and_policy.remote())\n",
    "    \n",
    "    return v, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to see the running time, and store the policy and state values to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Map:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "time: 1.7620351314544678\n",
      "\n",
      "State Value:\n",
      "\n",
      "[[  363.5    365.91   369.38   376.73   400.45   411.84   418.7    422.15]\n",
      " [  360.23   357.71   335.11   231.84   358.44   389.37   415.63   424.67]\n",
      " [  353.54   333.98   189.28 -1000.     173.63   227.87   389.72   428.36]\n",
      " [  339.81   306.39   224.98   -42.37    11.12 -1000.     269.05   436.45]\n",
      " [  290.57   139.43   -29.55 -1000.     -51.75    81.41   258.29   463.58]\n",
      " [  110.03 -1000.   -1000.    -309.32   -88.99    68.62 -1000.     498.87]\n",
      " [  -32.51 -1000.    -463.44  -463.41 -1000.     243.49 -1000.     720.18]\n",
      " [  -53.11  -187.7   -309.37 -1000.     262.9    625.53   734.24  1000.  ]]\n",
      "\n",
      "Policy:\n",
      "\n",
      "[[2 2 2 2 2 2 2 1]\n",
      " [3 3 3 3 3 2 2 1]\n",
      " [3 3 3 0 3 2 2 1]\n",
      " [3 0 0 0 3 0 2 1]\n",
      " [3 3 3 0 2 2 2 1]\n",
      " [3 0 0 2 2 1 0 1]\n",
      " [3 0 1 3 0 1 0 1]\n",
      " [3 0 0 0 2 2 2 0]]\n",
      "\n",
      "Average reward: 399.519\n",
      "\n",
      "Avereage discounted reward: 346.9153753922206\n",
      "\n",
      "State Value image view:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = 0.999\n",
    "env = FrozenLakeEnv(desc = MAP[0], is_slippery = True)\n",
    "print(\"Game Map:\")\n",
    "env.render()\n",
    "\n",
    "start_time = time.time()\n",
    "v, pi = sync_value_iteration_distributed(env, beta = beta, workers_num = 4)\n",
    "v_np, pi_np  = np.array(v), np.array(pi)\n",
    "end_time = time.time()\n",
    "run_time['Sync distributed'] = end_time - start_time\n",
    "print(\"time:\", run_time['Sync distributed'])\n",
    "print_results(v, pi, map_size, env, beta, 'dist_vi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
